{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# cross attention demo - used for encoder-decoder models (i.e., T5: Text-to-Text Transfer Transformer)"
      ],
      "metadata": {
        "id": "FcG-Kg_bu4ZT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Dummy encoder outputs\n",
        "encoder_outputs = torch.tensor([[1.0, 0.0, 0.5, 0.5],\n",
        "                                [0.5, 1.0, 0.0, 0.5],\n",
        "                                [0.0, 0.5, 1.0, 0.5]])"
      ],
      "metadata": {
        "id": "EnM8djHppyZH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder hidden state at current time step (query)\n",
        "decoder_hidden = torch.tensor([[0.5, 0.5, 0.0, 1.0]])"
      ],
      "metadata": {
        "id": "ySXUuthxp8-8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_hidden.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uq3OxpCGtRs9",
        "outputId": "44f013aa-a49f-4ba1-9e7f-2d14634583c0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Create Q, K, V ---\n",
        "W_q = nn.Linear(4, 4, bias=False)\n",
        "W_k = nn.Linear(4, 4, bias=False)\n",
        "W_v = nn.Linear(4, 4, bias=False)\n",
        "\n",
        "Q = W_q(decoder_hidden)\n",
        "K = W_k(encoder_outputs)\n",
        "V = W_v(encoder_outputs)"
      ],
      "metadata": {
        "id": "AweXUW5-qIzN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 2: Attention scores ---\n",
        "attn_scores = torch.matmul(Q, K.T)         # (1, 3)"
      ],
      "metadata": {
        "id": "KYidTu_JqRl3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 3: Softmax over encoder tokens ---\n",
        "attn_weights = F.softmax(attn_scores, dim=-1)  # (1, 3)"
      ],
      "metadata": {
        "id": "_EQfN_lwqUYT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 4: Compute attention context vector ---\n",
        "context = torch.matmul(attn_weights, V)    # (1, 4)"
      ],
      "metadata": {
        "id": "bwY6Eo1jqWgC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 5: Linear projection of context vector ---\n",
        "output_proj = nn.Linear(4, 4, bias=False)  # d_model â†’ d_model\n",
        "attn_output = output_proj(context)         # (1, 4)"
      ],
      "metadata": {
        "id": "gccrA8p4qY3O"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 6: Residual connection + LayerNorm ---\n",
        "residual = decoder_hidden\n",
        "layer_norm = nn.LayerNorm(4)\n",
        "output_embedding = layer_norm(attn_output + residual)  # (1, 4)"
      ],
      "metadata": {
        "id": "dBW2W7-Zqcak"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzgq6ewQnukv",
        "outputId": "826b5a7d-fee2-47b8-c6de-cf0b565b9385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Scores:\n",
            " tensor([[0.1654, 0.1047, 0.0918]], grad_fn=<MmBackward0>)\n",
            "Attention Weights:\n",
            " tensor([[0.3484, 0.3279, 0.3237]], grad_fn=<SoftmaxBackward0>)\n",
            "Context Vector:\n",
            " tensor([[ 0.7012, -0.0544,  0.0585,  0.2132]], grad_fn=<MmBackward0>)\n",
            "Final Output Embedding (for next layer):\n",
            " tensor([[-0.0396, -0.3584, -1.1777,  1.5756]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# --- Final Output ---\n",
        "print(\"Attention Scores:\\n\", attn_scores)\n",
        "print(\"Attention Weights:\\n\", attn_weights)\n",
        "print(\"Context Vector:\\n\", context)\n",
        "print(\"Final Output Embedding (for next layer):\\n\", output_embedding)"
      ]
    }
  ]
}